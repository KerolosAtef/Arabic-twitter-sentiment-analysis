{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(len(tf.config.list_physical_devices('GPU')))\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "import pyarabic.araby as ar\n",
    "\n",
    "# import Stemmer\n",
    "import functools, operator\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "Y-BpP8TZugwK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "seed=0"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "b90o2vElugwT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Arabic stop words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "Zr_8LPo0ugwX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "lJ0o2qs7ugwZ"
   },
   "outputs": [],
   "source": [
    "arabic_stop_words=[]\n",
    "with open ('list.txt',encoding='utf-8') as f :\n",
    "  for i in f.readlines() :\n",
    "    arabic_stop_words.append(i)\n",
    "    arabic_stop_words[-1]=arabic_stop_words[-1][:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: farasapy in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.14)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from farasapy) (4.64.0)\n",
      "Requirement already satisfied: requests in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from farasapy) (2.27.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->farasapy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->farasapy) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->farasapy) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->farasapy) (1.26.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->farasapy) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install farasapy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#============= Read CSV and apply data preperation =============#\n",
    "\n",
    "\n",
    "def data_preprocessing (data_frame):\n",
    "  # clean-up: remove #tags, http links and special symbols\n",
    "  data_frame['tweet']= data_frame['tweet'].apply(lambda x: x[2:-2])\n",
    "  data_frame['tweet']= data_frame['tweet'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "  data_frame['tweet'] = data_frame['tweet'].apply(lambda x: re.sub(r'[@|#]\\S*', '', x))\n",
    "  data_frame['tweet'] = data_frame['tweet'].apply(lambda x: re.sub(r'\"+', '', x))\n",
    "\n",
    "  # Remove arabic signs\n",
    "  data_frame['tweet'] = data_frame['tweet'].apply(lambda x: re.sub(r'([@A-Za-z0-9_ـــــــــــــ]+)|[^\\w\\s]|#|http\\S+', '', x))\n",
    "\n",
    "  # Remove repeated letters like \"الللللللللللللللله\" to \"الله\"\n",
    "  data_frame['tweet'] = data_frame['tweet'].apply(lambda x: x[0:2] + ''.join([x[i] for i in range(2, len(x)) if x[i]!=x[i-1] or x[i]!=x[i-2]]))\n",
    "\n",
    "  # remove stop words\n",
    "  data_frame['tweet'] = data_frame['tweet'].apply(lambda x: '' if x in arabic_stop_words else x)\n",
    "\n",
    "  from nltk.stem.isri import ISRIStemmer\n",
    "  df['tweet']=df['tweet'].apply(lambda x:ISRIStemmer().stem(x))\n",
    "\n",
    "  return data_frame\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "O7VPzTXYugwo"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "# st =  Stemmer.Stemmer('arabic')\n",
    "import string,emoji\n",
    "def data_cleaning (text):\n",
    "  text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "  text = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "  text = re.sub(r\"http\\S+\", \"\", text)\n",
    "  text = re.sub(r\"https\\S+\", \"\", text)\n",
    "  text = re.sub(r'\\s+', ' ', text)\n",
    "  text = re.sub(\"(\\s\\d+)\",\"\",text)\n",
    "  text = re.sub(r\"$\\d+\\W+|\\b\\d+\\b|\\W+\\d+$\", \"\", text)\n",
    "  text = re.sub(\"\\d+\", \" \", text)\n",
    "  text = ar.strip_tashkeel(text)\n",
    "  text = ar.strip_tatweel(text)\n",
    "  text = text.replace(\"#\", \" \");\n",
    "  text = text.replace(\"@\", \" \");\n",
    "  text = text.replace(\"_\", \" \");\n",
    "  translator = str.maketrans('', '', string.punctuation)\n",
    "  text = text.translate(translator)\n",
    "  em = text\n",
    "  em_split_emoji = emoji.get_emoji_regexp().split(em)\n",
    "  em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "  em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "  text = \" \".join(em_split)\n",
    "  text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "  # text_stem = \" \".join([st.stemWord(i) for i in text.split()])\n",
    "  # text = text +\" \"+ text_stem\n",
    "  text = text.replace(\"آ\", \"ا\")\n",
    "  text = text.replace(\"إ\", \"ا\")\n",
    "  text = text.replace(\"أ\", \"ا\")\n",
    "  text = text.replace(\"ؤ\", \"و\")\n",
    "  text = text.replace(\"ئ\", \"ي\")\n",
    "\n",
    "  return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df=data_preprocessing(df)\n",
    "df['tweet']=df['tweet'].apply(lambda x: data_cleaning(x))\n",
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "v_gDzrkBwMz6",
    "outputId": "bfc9d862-242a-4733-86db-bc2449555531",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kirolos Atef\\AppData\\Local\\Temp\\ipykernel_25240\\3188261332.py:20: DeprecationWarning: 'emoji.get_emoji_regexp()' is deprecated and will be removed in version 2.0.0. If you want to remove emoji from a string, consider the method emoji.replace_emoji(str, replace='').\n",
      "To hide this warning, pin/downgrade the package to 'emoji~=1.6.3'\n",
      "  em_split_emoji = emoji.get_emoji_regexp().split(em)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                  tweet class\n0     ان الذين يعيشون على الارض ليسوا ملايكة بل بشر ...   pos\n1                                    كل سنة وانتم طيبين   pos\n2                                 و انتهى مشوار الخواجة   neg\n3                             مش عارف ابتدى مذاكره منين   neg\n4     اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...   neg\n...                                                 ...   ...\n2054  الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...   neu\n2055  نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...   neu\n2056  ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...   pos\n2057                             انت متناقض جدا يا صلاح   neg\n2058     منطقة السيدة زينب ليلة المولد مسجد السيدة زينب   neu\n\n[2059 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ان الذين يعيشون على الارض ليسوا ملايكة بل بشر ...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>كل سنة وانتم طيبين</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>و انتهى مشوار الخواجة</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>مش عارف ابتدى مذاكره منين</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2054</th>\n      <td>الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>2055</th>\n      <td>نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>2056</th>\n      <td>ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>2057</th>\n      <td>انت متناقض جدا يا صلاح</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>2058</th>\n      <td>منطقة السيدة زينب ليلة المولد مسجد السيدة زينب</td>\n      <td>neu</td>\n    </tr>\n  </tbody>\n</table>\n<p>2059 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install farasapy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/aub-mind/arabert.git"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install pyarabic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [76]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01marabert\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocess\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ArabertPreprocessor\n\u001B[0;32m      3\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maubmindlab/bert-base-arabertv2\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 4\u001B[0m arabert_prep \u001B[38;5;241m=\u001B[39m \u001B[43mArabertPreprocessor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# df['tweet']=df['tweet'].apply(lambda x: arabert_prep.preprocess(x))\u001B[39;00m\n\u001B[0;32m      8\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mولن نبالغ إذا قلنا: إن \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mهاتف\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m أو \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mكمبيوتر المكتب\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m في زمننا هذا ضروري\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32mE:\\Kero\\Notebook\\Arabic NLP\\arabert\\preprocess.py:176\u001B[0m, in \u001B[0;36mArabertPreprocessor.__init__\u001B[1;34m(self, model_name, remove_html_markup, replace_urls_emails_mentions, strip_tashkeel, strip_tatweel, insert_white_spaces, remove_non_digit_repetition, keep_emojis, replace_slash_with_dash, map_hindi_numbers_to_arabic, apply_farasa_segmentation)\u001B[0m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    174\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfarasa\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msegmenter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FarasaSegmenter\n\u001B[1;32m--> 176\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfarasa_segmenter \u001B[38;5;241m=\u001B[39m \u001B[43mFarasaSegmenter\u001B[49m\u001B[43m(\u001B[49m\u001B[43minteractive\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m:\n\u001B[0;32m    178\u001B[0m     logging\u001B[38;5;241m.\u001B[39merror(\n\u001B[0;32m    179\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfarasapy is not installed, you want be able to process text for AraBERTv1 and v2. Install it using: pip install farasapy\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    180\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\farasa\\__base.py:41\u001B[0m, in \u001B[0;36mFarasaBase.__init__\u001B[1;34m(self, interactive, logging_level)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mperform system check...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheck java version...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 41\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_java_version\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheck toolkit binaries...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_toolkit_binaries()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\farasa\\__base.py:144\u001B[0m, in \u001B[0;36mFarasaBase._check_java_version\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_java_version\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    143\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 144\u001B[0m         version_proc_output \u001B[38;5;241m=\u001B[39m \u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_output\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    145\u001B[0m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mjava\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m-version\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstderr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSTDOUT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m    146\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    147\u001B[0m         \u001B[38;5;66;03m# version_pattern = r\"\\\"(\\d+\\.\\d+).*\\\"\"\u001B[39;00m\n\u001B[0;32m    148\u001B[0m         version_pattern \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;124m(\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md+(\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md+)\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m0,1})\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py:420\u001B[0m, in \u001B[0;36mcheck_output\u001B[1;34m(timeout, *popenargs, **kwargs)\u001B[0m\n\u001B[0;32m    417\u001B[0m         empty \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    418\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m empty\n\u001B[1;32m--> 420\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m run(\u001B[38;5;241m*\u001B[39mpopenargs, stdout\u001B[38;5;241m=\u001B[39mPIPE, timeout\u001B[38;5;241m=\u001B[39mtimeout, check\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    421\u001B[0m            \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\u001B[38;5;241m.\u001B[39mstdout\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py:501\u001B[0m, in \u001B[0;36mrun\u001B[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001B[0m\n\u001B[0;32m    498\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstdout\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m PIPE\n\u001B[0;32m    499\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstderr\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m PIPE\n\u001B[1;32m--> 501\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Popen(\u001B[38;5;241m*\u001B[39mpopenargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;28;01mas\u001B[39;00m process:\n\u001B[0;32m    502\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    503\u001B[0m         stdout, stderr \u001B[38;5;241m=\u001B[39m process\u001B[38;5;241m.\u001B[39mcommunicate(\u001B[38;5;28minput\u001B[39m, timeout\u001B[38;5;241m=\u001B[39mtimeout)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py:966\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001B[0m\n\u001B[0;32m    962\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_mode:\n\u001B[0;32m    963\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mTextIOWrapper(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr,\n\u001B[0;32m    964\u001B[0m                     encoding\u001B[38;5;241m=\u001B[39mencoding, errors\u001B[38;5;241m=\u001B[39merrors)\n\u001B[1;32m--> 966\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execute_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecutable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreexec_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclose_fds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    967\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mpass_fds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcwd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    968\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mstartupinfo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreationflags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshell\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    969\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mp2cread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp2cwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    970\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mc2pread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc2pwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    971\u001B[0m \u001B[43m                        \u001B[49m\u001B[43merrread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    972\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mrestore_signals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    973\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mgid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mumask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    974\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mstart_new_session\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    975\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m    976\u001B[0m     \u001B[38;5;66;03m# Cleanup if the child failed starting.\u001B[39;00m\n\u001B[0;32m    977\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mfilter\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m, (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstdin, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstdout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr)):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py:1435\u001B[0m, in \u001B[0;36mPopen._execute_child\u001B[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001B[0m\n\u001B[0;32m   1433\u001B[0m \u001B[38;5;66;03m# Start the process\u001B[39;00m\n\u001B[0;32m   1434\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1435\u001B[0m     hp, ht, pid, tid \u001B[38;5;241m=\u001B[39m \u001B[43m_winapi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCreateProcess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexecutable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1436\u001B[0m \u001B[43m                             \u001B[49m\u001B[38;5;66;43;03m# no special security\u001B[39;49;00m\n\u001B[0;32m   1437\u001B[0m \u001B[43m                             \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1438\u001B[0m \u001B[43m                             \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mclose_fds\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1439\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mcreationflags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1440\u001B[0m \u001B[43m                             \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1441\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mcwd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1442\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mstartupinfo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1443\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   1444\u001B[0m     \u001B[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001B[39;00m\n\u001B[0;32m   1445\u001B[0m     \u001B[38;5;66;03m# handles that only the child should have open.  You need\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1448\u001B[0m     \u001B[38;5;66;03m# pipe will not close when the child process exits and the\u001B[39;00m\n\u001B[0;32m   1449\u001B[0m     \u001B[38;5;66;03m# ReadFile will hang.\u001B[39;00m\n\u001B[0;32m   1450\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001B[0;32m   1451\u001B[0m                          c2pread, c2pwrite,\n\u001B[0;32m   1452\u001B[0m                          errread, errwrite)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "model_name = \"UBC-NLP/MARBERT\"\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "\n",
    "df['tweet']=df['tweet'].apply(lambda x: arabert_prep.preprocess(x))\n",
    "\n",
    "\n",
    "# text = \"ولن نبالغ إذا قلنا: إن 'هاتف' أو 'كمبيوتر المكتب' في زمننا هذا ضروري\"\n",
    "# arabert_prep.preprocess(text)\n",
    "# # \"و+ لن نبالغ إذا قل +نا : إن ' هاتف ' أو ' كمبيوتر ال+ مكتب ' في زمن +نا هذا ضروري\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Label Encoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "tBEVTgBRugws"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  tweet  class\n0     ان الذين يعيشون على الارض ليسوا ملايكة بل بشر ...      2\n1                                    كل سنة وانتم طيبين      2\n2                                 و انتهى مشوار الخواجة      0\n3                             مش عارف ابتدى مذاكره منين      0\n4     اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...      0\n...                                                 ...    ...\n2054  الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...      1\n2055  نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...      1\n2056  ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...      2\n2057                             انت متناقض جدا يا صلاح      0\n2058     منطقة السيدة زينب ليلة المولد مسجد السيدة زينب      1\n\n[2059 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ان الذين يعيشون على الارض ليسوا ملايكة بل بشر ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>كل سنة وانتم طيبين</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>و انتهى مشوار الخواجة</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>مش عارف ابتدى مذاكره منين</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2054</th>\n      <td>الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2055</th>\n      <td>نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2056</th>\n      <td>ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2057</th>\n      <td>انت متناقض جدا يا صلاح</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2058</th>\n      <td>منطقة السيدة زينب ليلة المولد مسجد السيدة زينب</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2059 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "# Apply label encoding over the labels\n",
    "lable_encoder = preprocessing.LabelEncoder()\n",
    "encoded_labels =lable_encoder.fit_transform(df[\"class\"])\n",
    "df['class']=encoded_labels\n",
    "df"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "nvWAmR5nugwu",
    "outputId": "96f914bc-922a-413b-d765-f85b8f28dd57"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "34"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length']=df['tweet'].apply(lambda x:len(x.split(' ')))\n",
    "df['length'].max()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "      Unnamed: 0                                              tweet  class\n0              0  ' # علمتني _ الحياه أن الذين يعيشون على الأرض ...      2\n1              1       ' # ميري _ كرسمس كل سنة وانتم طيبين [رابط] '      2\n2              2                          ' و انتهى مشوار الخواجة '      0\n3              3                  ' مش عارف ابتدى مذاكره منين : - '      0\n4              4  ' [مستخدم] إختصروا الطريق بدلا من إختيار المنص...      0\n...          ...                                                ...    ...\n2054        2054  ' [مستخدم] الجمال مبيحتاح اي مكياج لناعم وله خ...      1\n2055        2055  ' [مستخدم] نتمني وجود الفنانة رنا سماحة افضل ف...      1\n2056        2056  ' ولد الهدى فالكائنات ضياء . . وفم الزمان تبسم...      2\n2057        2057       ' [مستخدم] [مستخدم] انت متناقض جدا يا صلاح '      0\n2058        2058  ' منطقة السيدة زينب ليلة المولد @ مسجد السيدة ...      1\n\n[2059 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>tweet</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>' # علمتني _ الحياه أن الذين يعيشون على الأرض ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>' # ميري _ كرسمس كل سنة وانتم طيبين [رابط] '</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>' و انتهى مشوار الخواجة '</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>' مش عارف ابتدى مذاكره منين : - '</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>' [مستخدم] إختصروا الطريق بدلا من إختيار المنص...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2054</th>\n      <td>2054</td>\n      <td>' [مستخدم] الجمال مبيحتاح اي مكياج لناعم وله خ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2055</th>\n      <td>2055</td>\n      <td>' [مستخدم] نتمني وجود الفنانة رنا سماحة افضل ف...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2056</th>\n      <td>2056</td>\n      <td>' ولد الهدى فالكائنات ضياء . . وفم الزمان تبسم...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2057</th>\n      <td>2057</td>\n      <td>' [مستخدم] [مستخدم] انت متناقض جدا يا صلاح '</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2058</th>\n      <td>2058</td>\n      <td>' منطقة السيدة زينب ليلة المولد @ مسجد السيدة ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2059 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('preprocessed_train_large_bert.csv')\n",
    "seed=0\n",
    "from sklearn import preprocessing\n",
    "# Apply label encoding over the labels\n",
    "lable_encoder = preprocessing.LabelEncoder()\n",
    "encoded_labels =lable_encoder.fit_transform(df[\"class\"])\n",
    "df['class']=encoded_labels\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Test Split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "RscS77J9ugww"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "1826    ' هتفضل مشاكلي بيني و بين نفسي لحد ما اموت و ه...\n1505       ' لو فشلنا ادينا حاولنا دا هيدي الامل لغيرنا '\n1994    ' اذا جاء أجل الله لايقدم ولا يؤخر - الموت اقر...\n1349               ' دايما بتاخد مننا اعز واغلى ماعندنا '\n781     ' # لو _ سانتا _ كلوز _ مصري كآن هيلعن اليوم ا...\n                              ...                        \n596                                ' امراه رائعه [رابط] '\n1887            ' مارلين مونرو حلوة فشخ ! . . ليه كدا ! '\n942      ' # موطني _ هو تحت البطانيه و جنب شاحن الموبيل '\n634     ' [مستخدم] [مستخدم] [مستخدم] [مستخدم] @ [رابط]...\n19      ' [مستخدم] ها ايه رايك اقوله ايه بقا . . انا ب...\nName: tweet, Length: 412, dtype: object"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation=train_test_split(df['tweet'], df['class'], test_size=0.2, random_state=seed)\n",
    "X_validation"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhLxzIXvugwx",
    "outputId": "418c9aa9-789d-4708-ed5e-bb960fe2514b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TF_IDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "kevPnoxdugwy"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tfidf_ngram(n_gram,X_train,X_val):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(n_gram,n_gram))\n",
    "    x_train_vec = vectorizer.fit_transform(X_train)\n",
    "    x_test_vec = vectorizer.transform(X_val)\n",
    "    return x_train_vec,x_test_vec"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "roXYHxrdugwy"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Applying tfidf with 1-gram, 2-gram and 3-gram\n",
    "tfidf_1g_transformation_train,tfidf_1g_transformation_validation= tfidf_ngram(1,X_train,X_validation)\n",
    "tfidf_2g_transformation_train,tfidf_2g_transformation_validation= tfidf_ngram(2,X_train,X_validation)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "iuWEPExnugwz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Machine learning models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "7BYrD0RLugw0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.994535519125683\n",
      "0.3859223300970874\n",
      "0.5343047965998786\n",
      "0.3616504854368932\n",
      "0.9963570127504554\n",
      "0.42961165048543687\n",
      "0.9963570127504554\n",
      "0.41262135922330095\n",
      "0.9914996964177292\n",
      "0.39563106796116504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "models=[SVC(),XGBClassifier(),RandomForestClassifier(),DecisionTreeClassifier(),LogisticRegression()]\n",
    "for m in models :\n",
    "    m.fit(tfidf_2g_transformation_train,y_train)\n",
    "    print(m.score(tfidf_2g_transformation_train,y_train))\n",
    "    print(m.score(tfidf_2g_transformation_validation,y_validation))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RCB9CYrwugw0",
    "outputId": "6f138f26-bddf-43cf-c2ca-8af7b36cefdd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model and Tokenizer initialization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "546gZdstugw1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.19.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: requests in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PPa1X-Tugw1",
    "outputId": "cbb39e64-a238-4dc1-c0e4-779c2fff682c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at UBC-NLP/MARBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "#============= Initialize Arabic Bert =============#\n",
    "#load your pre_trained model with all its weights\n",
    "# model_name= 'aubmindlab/bert-base-arabertv02'\n",
    "model_name='UBC-NLP/MARBERT' #top\n",
    "# model_name='asafaya/bert-base-arabic'\n",
    "# model_name='AraBERTv0.2-Twitter-base'\n",
    "# model_name='aubmindlab/bert-large-arabertv2'\n",
    "# model_name='aubmindlab/bert-base-arabertv02-twitter'\n",
    "# model_name='aubmindlab/bert-large-arabertv02-twitter'\n",
    "# model_name='aubmindlab/aragpt2-base'\n",
    "\n",
    "# model_name='aubmindlab/bert-base-arabertv2'\n",
    "tokenizer =AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "# model=AutoModel.from_pretrained(model_name,output_hidden_states=True)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WqHEVmquugw1",
    "outputId": "0200298a-c7e6-419e-aea0-f6260a11b95e"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'git' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/aub-mind/arabert.git"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarabic in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.6.15)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyarabic) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarabic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Tokenize the sentences using bert tokenizer\n",
    "df[\"bert_tokens\"] = df.tweet.apply(lambda x: tokenizer(x).tokens())\n",
    "df[\"bert_tokens_ids\"] = df.tweet.apply(lambda x: tokenizer(x).tokens())"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "sc5VRJOCugw2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "      Unnamed: 0                                              tweet  class  \\\n0              0  ' # علمتني _ الحياه أن الذين يعيشون على الأرض ...      2   \n1              1       ' # ميري _ كرسمس كل سنة وانتم طيبين [رابط] '      2   \n2              2                          ' و انتهى مشوار الخواجة '      0   \n3              3                  ' مش عارف ابتدى مذاكره منين : - '      0   \n4              4  ' [مستخدم] إختصروا الطريق بدلا من إختيار المنص...      0   \n...          ...                                                ...    ...   \n2054        2054  ' [مستخدم] الجمال مبيحتاح اي مكياج لناعم وله خ...      1   \n2055        2055  ' [مستخدم] نتمني وجود الفنانة رنا سماحة افضل ف...      1   \n2056        2056  ' ولد الهدى فالكائنات ضياء . . وفم الزمان تبسم...      2   \n2057        2057       ' [مستخدم] [مستخدم] انت متناقض جدا يا صلاح '      0   \n2058        2058  ' منطقة السيدة زينب ليلة المولد @ مسجد السيدة ...      1   \n\n                                            bert_tokens  \\\n0     [[CLS], ', [UNK], علمتني, [UNK], الحياه, ان, ا...   \n1     [[CLS], ', [UNK], ميري, [UNK], كرس, ##مس, كل, ...   \n2        [[CLS], ', و, انتهى, مشوار, الخواجة, ', [SEP]]   \n3     [[CLS], ', مش, عارف, ابتدى, مذاكره, منين, :, -...   \n4     [[CLS], ', [, مستخدم, ], اختصر, ##وا, الطريق, ...   \n...                                                 ...   \n2054  [[CLS], ', [, مستخدم, ], الجمال, مبيح, ##تاح, ...   \n2055  [[CLS], ', [, مستخدم, ], نتمني, وجود, الفنانة,...   \n2056  [[CLS], ', ولد, الهدى, فالك, ##اينات, ضياء, .,...   \n2057  [[CLS], ', [, مستخدم, ], [, مستخدم, ], انت, مت...   \n2058  [[CLS], ', منطقة, السيدة, زينب, ليلة, المولد, ...   \n\n                                        bert_tokens_ids  \n0     [[CLS], ', [UNK], علمتني, [UNK], الحياه, ان, ا...  \n1     [[CLS], ', [UNK], ميري, [UNK], كرس, ##مس, كل, ...  \n2        [[CLS], ', و, انتهى, مشوار, الخواجة, ', [SEP]]  \n3     [[CLS], ', مش, عارف, ابتدى, مذاكره, منين, :, -...  \n4     [[CLS], ', [, مستخدم, ], اختصر, ##وا, الطريق, ...  \n...                                                 ...  \n2054  [[CLS], ', [, مستخدم, ], الجمال, مبيح, ##تاح, ...  \n2055  [[CLS], ', [, مستخدم, ], نتمني, وجود, الفنانة,...  \n2056  [[CLS], ', ولد, الهدى, فالك, ##اينات, ضياء, .,...  \n2057  [[CLS], ', [, مستخدم, ], [, مستخدم, ], انت, مت...  \n2058  [[CLS], ', منطقة, السيدة, زينب, ليلة, المولد, ...  \n\n[2059 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>tweet</th>\n      <th>class</th>\n      <th>bert_tokens</th>\n      <th>bert_tokens_ids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>' # علمتني _ الحياه أن الذين يعيشون على الأرض ...</td>\n      <td>2</td>\n      <td>[[CLS], ', [UNK], علمتني, [UNK], الحياه, ان, ا...</td>\n      <td>[[CLS], ', [UNK], علمتني, [UNK], الحياه, ان, ا...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>' # ميري _ كرسمس كل سنة وانتم طيبين [رابط] '</td>\n      <td>2</td>\n      <td>[[CLS], ', [UNK], ميري, [UNK], كرس, ##مس, كل, ...</td>\n      <td>[[CLS], ', [UNK], ميري, [UNK], كرس, ##مس, كل, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>' و انتهى مشوار الخواجة '</td>\n      <td>0</td>\n      <td>[[CLS], ', و, انتهى, مشوار, الخواجة, ', [SEP]]</td>\n      <td>[[CLS], ', و, انتهى, مشوار, الخواجة, ', [SEP]]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>' مش عارف ابتدى مذاكره منين : - '</td>\n      <td>0</td>\n      <td>[[CLS], ', مش, عارف, ابتدى, مذاكره, منين, :, -...</td>\n      <td>[[CLS], ', مش, عارف, ابتدى, مذاكره, منين, :, -...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>' [مستخدم] إختصروا الطريق بدلا من إختيار المنص...</td>\n      <td>0</td>\n      <td>[[CLS], ', [, مستخدم, ], اختصر, ##وا, الطريق, ...</td>\n      <td>[[CLS], ', [, مستخدم, ], اختصر, ##وا, الطريق, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2054</th>\n      <td>2054</td>\n      <td>' [مستخدم] الجمال مبيحتاح اي مكياج لناعم وله خ...</td>\n      <td>1</td>\n      <td>[[CLS], ', [, مستخدم, ], الجمال, مبيح, ##تاح, ...</td>\n      <td>[[CLS], ', [, مستخدم, ], الجمال, مبيح, ##تاح, ...</td>\n    </tr>\n    <tr>\n      <th>2055</th>\n      <td>2055</td>\n      <td>' [مستخدم] نتمني وجود الفنانة رنا سماحة افضل ف...</td>\n      <td>1</td>\n      <td>[[CLS], ', [, مستخدم, ], نتمني, وجود, الفنانة,...</td>\n      <td>[[CLS], ', [, مستخدم, ], نتمني, وجود, الفنانة,...</td>\n    </tr>\n    <tr>\n      <th>2056</th>\n      <td>2056</td>\n      <td>' ولد الهدى فالكائنات ضياء . . وفم الزمان تبسم...</td>\n      <td>2</td>\n      <td>[[CLS], ', ولد, الهدى, فالك, ##اينات, ضياء, .,...</td>\n      <td>[[CLS], ', ولد, الهدى, فالك, ##اينات, ضياء, .,...</td>\n    </tr>\n    <tr>\n      <th>2057</th>\n      <td>2057</td>\n      <td>' [مستخدم] [مستخدم] انت متناقض جدا يا صلاح '</td>\n      <td>0</td>\n      <td>[[CLS], ', [, مستخدم, ], [, مستخدم, ], انت, مت...</td>\n      <td>[[CLS], ', [, مستخدم, ], [, مستخدم, ], انت, مت...</td>\n    </tr>\n    <tr>\n      <th>2058</th>\n      <td>2058</td>\n      <td>' منطقة السيدة زينب ليلة المولد @ مسجد السيدة ...</td>\n      <td>1</td>\n      <td>[[CLS], ', منطقة, السيدة, زينب, ليلة, المولد, ...</td>\n      <td>[[CLS], ', منطقة, السيدة, زينب, ليلة, المولد, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2059 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "RkTkQeWbugw2",
    "outputId": "73f38be7-080e-49d8-ff9c-2d528830377d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "df[\"encoded\"] = df.tweet.apply(lambda x: tokenizer.encode_plus(x,return_tensors='pt')['input_ids'])"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "0xw8-QmIugw3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "      Unnamed: 0                                              tweet  class  \\\n0              0  ' # علمتني _ الحياه أن الذين يعيشون على الأرض ...      2   \n1              1       ' # ميري _ كرسمس كل سنة وانتم طيبين [رابط] '      2   \n2              2                          ' و انتهى مشوار الخواجة '      0   \n3              3                  ' مش عارف ابتدى مذاكره منين : - '      0   \n4              4  ' [مستخدم] إختصروا الطريق بدلا من إختيار المنص...      0   \n...          ...                                                ...    ...   \n2054        2054  ' [مستخدم] الجمال مبيحتاح اي مكياج لناعم وله خ...      1   \n2055        2055  ' [مستخدم] نتمني وجود الفنانة رنا سماحة افضل ف...      1   \n2056        2056  ' ولد الهدى فالكائنات ضياء . . وفم الزمان تبسم...      2   \n2057        2057       ' [مستخدم] [مستخدم] انت متناقض جدا يا صلاح '      0   \n2058        2058  ' منطقة السيدة زينب ليلة المولد @ مسجد السيدة ...      1   \n\n                                            bert_tokens  \\\n0     [[CLS], ', [UNK], علمتني, [UNK], الحياه, ان, ا...   \n1     [[CLS], ', [UNK], ميري, [UNK], كرس, ##مس, كل, ...   \n2        [[CLS], ', و, انتهى, مشوار, الخواجة, ', [SEP]]   \n3     [[CLS], ', مش, عارف, ابتدى, مذاكره, منين, :, -...   \n4     [[CLS], ', [, مستخدم, ], اختصر, ##وا, الطريق, ...   \n...                                                 ...   \n2054  [[CLS], ', [, مستخدم, ], الجمال, مبيح, ##تاح, ...   \n2055  [[CLS], ', [, مستخدم, ], نتمني, وجود, الفنانة,...   \n2056  [[CLS], ', ولد, الهدى, فالك, ##اينات, ضياء, .,...   \n2057  [[CLS], ', [, مستخدم, ], [, مستخدم, ], انت, مت...   \n2058  [[CLS], ', منطقة, السيدة, زينب, ليلة, المولد, ...   \n\n                                        bert_tokens_ids  \\\n0     [[CLS], ', [UNK], علمتني, [UNK], الحياه, ان, ا...   \n1     [[CLS], ', [UNK], ميري, [UNK], كرس, ##مس, كل, ...   \n2        [[CLS], ', و, انتهى, مشوار, الخواجة, ', [SEP]]   \n3     [[CLS], ', مش, عارف, ابتدى, مذاكره, منين, :, -...   \n4     [[CLS], ', [, مستخدم, ], اختصر, ##وا, الطريق, ...   \n...                                                 ...   \n2054  [[CLS], ', [, مستخدم, ], الجمال, مبيح, ##تاح, ...   \n2055  [[CLS], ', [, مستخدم, ], نتمني, وجود, الفنانة,...   \n2056  [[CLS], ', ولد, الهدى, فالك, ##اينات, ضياء, .,...   \n2057  [[CLS], ', [, مستخدم, ], [, مستخدم, ], انت, مت...   \n2058  [[CLS], ', منطقة, السيدة, زينب, ليلة, المولد, ...   \n\n                                                encoded  \n0     [[tensor(2), tensor(9), tensor(1), tensor(8244...  \n1     [[tensor(2), tensor(9), tensor(1), tensor(5390...  \n2     [[tensor(2), tensor(9), tensor(144), tensor(76...  \n3     [[tensor(2), tensor(9), tensor(2093), tensor(3...  \n4     [[tensor(2), tensor(9), tensor(35), tensor(789...  \n...                                                 ...  \n2054  [[tensor(2), tensor(9), tensor(35), tensor(789...  \n2055  [[tensor(2), tensor(9), tensor(35), tensor(789...  \n2056  [[tensor(2), tensor(9), tensor(3735), tensor(4...  \n2057  [[tensor(2), tensor(9), tensor(35), tensor(789...  \n2058  [[tensor(2), tensor(9), tensor(5627), tensor(1...  \n\n[2059 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>tweet</th>\n      <th>class</th>\n      <th>bert_tokens</th>\n      <th>bert_tokens_ids</th>\n      <th>encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>' # علمتني _ الحياه أن الذين يعيشون على الأرض ...</td>\n      <td>2</td>\n      <td>[[CLS], ', [UNK], علمتني, [UNK], الحياه, ان, ا...</td>\n      <td>[[CLS], ', [UNK], علمتني, [UNK], الحياه, ان, ا...</td>\n      <td>[[tensor(2), tensor(9), tensor(1), tensor(8244...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>' # ميري _ كرسمس كل سنة وانتم طيبين [رابط] '</td>\n      <td>2</td>\n      <td>[[CLS], ', [UNK], ميري, [UNK], كرس, ##مس, كل, ...</td>\n      <td>[[CLS], ', [UNK], ميري, [UNK], كرس, ##مس, كل, ...</td>\n      <td>[[tensor(2), tensor(9), tensor(1), tensor(5390...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>' و انتهى مشوار الخواجة '</td>\n      <td>0</td>\n      <td>[[CLS], ', و, انتهى, مشوار, الخواجة, ', [SEP]]</td>\n      <td>[[CLS], ', و, انتهى, مشوار, الخواجة, ', [SEP]]</td>\n      <td>[[tensor(2), tensor(9), tensor(144), tensor(76...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>' مش عارف ابتدى مذاكره منين : - '</td>\n      <td>0</td>\n      <td>[[CLS], ', مش, عارف, ابتدى, مذاكره, منين, :, -...</td>\n      <td>[[CLS], ', مش, عارف, ابتدى, مذاكره, منين, :, -...</td>\n      <td>[[tensor(2), tensor(9), tensor(2093), tensor(3...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>' [مستخدم] إختصروا الطريق بدلا من إختيار المنص...</td>\n      <td>0</td>\n      <td>[[CLS], ', [, مستخدم, ], اختصر, ##وا, الطريق, ...</td>\n      <td>[[CLS], ', [, مستخدم, ], اختصر, ##وا, الطريق, ...</td>\n      <td>[[tensor(2), tensor(9), tensor(35), tensor(789...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2054</th>\n      <td>2054</td>\n      <td>' [مستخدم] الجمال مبيحتاح اي مكياج لناعم وله خ...</td>\n      <td>1</td>\n      <td>[[CLS], ', [, مستخدم, ], الجمال, مبيح, ##تاح, ...</td>\n      <td>[[CLS], ', [, مستخدم, ], الجمال, مبيح, ##تاح, ...</td>\n      <td>[[tensor(2), tensor(9), tensor(35), tensor(789...</td>\n    </tr>\n    <tr>\n      <th>2055</th>\n      <td>2055</td>\n      <td>' [مستخدم] نتمني وجود الفنانة رنا سماحة افضل ف...</td>\n      <td>1</td>\n      <td>[[CLS], ', [, مستخدم, ], نتمني, وجود, الفنانة,...</td>\n      <td>[[CLS], ', [, مستخدم, ], نتمني, وجود, الفنانة,...</td>\n      <td>[[tensor(2), tensor(9), tensor(35), tensor(789...</td>\n    </tr>\n    <tr>\n      <th>2056</th>\n      <td>2056</td>\n      <td>' ولد الهدى فالكائنات ضياء . . وفم الزمان تبسم...</td>\n      <td>2</td>\n      <td>[[CLS], ', ولد, الهدى, فالك, ##اينات, ضياء, .,...</td>\n      <td>[[CLS], ', ولد, الهدى, فالك, ##اينات, ضياء, .,...</td>\n      <td>[[tensor(2), tensor(9), tensor(3735), tensor(4...</td>\n    </tr>\n    <tr>\n      <th>2057</th>\n      <td>2057</td>\n      <td>' [مستخدم] [مستخدم] انت متناقض جدا يا صلاح '</td>\n      <td>0</td>\n      <td>[[CLS], ', [, مستخدم, ], [, مستخدم, ], انت, مت...</td>\n      <td>[[CLS], ', [, مستخدم, ], [, مستخدم, ], انت, مت...</td>\n      <td>[[tensor(2), tensor(9), tensor(35), tensor(789...</td>\n    </tr>\n    <tr>\n      <th>2058</th>\n      <td>2058</td>\n      <td>' منطقة السيدة زينب ليلة المولد @ مسجد السيدة ...</td>\n      <td>1</td>\n      <td>[[CLS], ', منطقة, السيدة, زينب, ليلة, المولد, ...</td>\n      <td>[[CLS], ', منطقة, السيدة, زينب, ليلة, المولد, ...</td>\n      <td>[[tensor(2), tensor(9), tensor(5627), tensor(1...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2059 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "34QQvxBhugw3",
    "outputId": "7a4279c0-81a7-4fd3-e22a-158fffd8d130"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Padding and attention mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "fhb0-jQMugw4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway.\n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 64\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in df['bert_tokens']]\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "taCZLE63ugw4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, encoded_labels,\n",
    "                                                            random_state=seed, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=seed, test_size=0.1)\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 64\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop,\n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ip_wd-t9ugw5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set optimizer parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "CEvllsdYugw5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],'weight_decay_rate': 0.01},\n",
    "                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],'weight_decay_rate': 0.0}]\n",
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "# optimizer = optim.BertAdam(optimizer_grouped_parameters,lr=2e-5,warmup=.1)\n",
    "optimizer = optim.AdamW(optimizer_grouped_parameters,lr=5e-6)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "SIutfqrnugw5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "sZZCYaUKugw5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.0659035279833038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   9%|▉         | 1/11 [00:12<02:09, 12.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5887276785714286\n",
      "Train loss: 0.8915698384416515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  18%|█▊        | 2/11 [00:24<01:46, 11.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6746651785714286\n",
      "Train loss: 0.6509025322979894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  27%|██▋       | 3/11 [00:35<01:32, 11.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7176339285714286\n",
      "Train loss: 0.4922829547832752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  36%|███▋      | 4/11 [00:46<01:20, 11.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7589285714285714\n",
      "Train loss: 0.37091002587614386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  45%|████▌     | 5/11 [00:57<01:08, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7511160714285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  45%|████▌     | 5/11 [00:59<01:11, 11.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36m<cell line: 20>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     48\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     51\u001B[0m \u001B[38;5;66;03m# Update tracking variables\u001B[39;00m\n\u001B[1;32m---> 52\u001B[0m tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     53\u001B[0m nb_tr_examples \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m b_input_ids\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     54\u001B[0m nb_tr_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "import numpy as np\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "t = []\n",
    "\n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 11\n",
    "\n",
    "# Transfer the model to GPU\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "\n",
    "  # Training\n",
    "\n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  model.train()\n",
    "\n",
    "  # Tracking variables\n",
    "  tr_loss = 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    b_labels = b_labels.type(torch.LongTensor)   # casting to long\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    loss = model(b_input_ids.to(\"cuda\"), token_type_ids=None, attention_mask=b_input_mask.to(\"cuda\"), labels=b_labels.to(\"cuda\"))[\"loss\"]\n",
    "    train_loss_set.append(loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables\n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    # batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    b_labels = b_labels.type(torch.LongTensor)   # casting to long\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits = model(b_input_ids.to(\"cuda\"), token_type_ids=None, attention_mask=b_input_mask.to(\"cuda\"))\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits[\"logits\"].detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0kfOowbxugw6",
    "outputId": "55691992-c8a4-4b2d-9e61-1074177eb38b"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "#============= Read CSV and apply data preperation =============#\n",
    "df_submit = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# # clean-up: remove #tags, http links and special symbols\n",
    "# df_submit.tweet = df_submit.tweet.apply(lambda x: x[2:-2])\n",
    "# df_submit.tweet = df_submit.tweet.apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "# df_submit.tweet = df_submit.tweet.apply(lambda x: re.sub(r'[@|#]\\S*', '', x))\n",
    "# df_submit.tweet = df_submit.tweet.apply(lambda x: re.sub(r'\"+', '', x))\n",
    "#\n",
    "# # Remove arabic signs\n",
    "# df_submit.tweet = df_submit.tweet.apply(lambda x: re.sub(r'([@A-Za-z0-9_ـــــــــــــ]+)|[^\\w\\s]|#|http\\S+', '', x))\n",
    "#\n",
    "# # Remove repeated letters like \"الللللللللللللللله\" to \"الله\"\n",
    "# df_submit.tweet = df_submit.tweet.apply(lambda x: x[0:2] + ''.join([x[i] for i in range(2, len(x)) if x[i]!=x[i-1] or x[i]!=x[i-2]]))\n",
    "#\n",
    "# # remove stop words\n",
    "# df_submit.iloc[:,0] = df_submit.iloc[:,0].apply(lambda x: '' if x in arabic_stop_words else x)\n",
    "\n",
    "\n",
    "# df_submit=data_preprocessing(df_submit)\n",
    "#\n",
    "# df_submit[\"tweet\"] = df_submit.tweet.apply(lambda x: data_cleaning(x))\n",
    "\n",
    "df_submit=pd.read_csv('preprocessed_test_UBC.csv')\n",
    "\n",
    "# Tokenize the sentences using bert tokenizer\n",
    "df_submit[\"bert_tokens\"] = df_submit.tweet.apply(lambda x: tokenizer(x).tokens())"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "yJ2XSmwSugw6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "bert_tokens_submit = df_submit[\"bert_tokens\"]"
   ],
   "metadata": {
    "id": "G7iPpop4ytzT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 64\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids_submit = [tokenizer.convert_tokens_to_ids(x) for x in bert_tokens_submit]\n",
    "# Pad our input tokens\n",
    "input_ids_submit = pad_sequences(input_ids_submit, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks_submit = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids_submit:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks_submit.append(seq_mask)"
   ],
   "metadata": {
    "id": "Z1vJgu5f0A-3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "inputs_submit = torch.tensor(input_ids_submit)\n",
    "masks_submit = torch.tensor(attention_masks_submit)\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "batch_size = 64\n",
    "submit_data = TensorDataset(inputs_submit, masks_submit)\n",
    "\n",
    "# do not use shuffle, we need the preds to be in same order\n",
    "submit_dataloader = DataLoader(submit_data, batch_size=batch_size)#, shuffle=True)"
   ],
   "metadata": {
    "id": "HUeIG5WD0M9n",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Put the model in an evaluation state\n",
    "model.eval()\n",
    "\n",
    "# Transfer model to GPU\n",
    "model.to(\"cuda\")\n",
    "\n",
    "outputs = []\n",
    "for input, masks in submit_dataloader:\n",
    "  torch.cuda.empty_cache() # empty the gpu memory\n",
    "\n",
    "  # Transfer the batch to gpu\n",
    "  input = input.to('cuda')\n",
    "  masks = masks.to('cuda')\n",
    "\n",
    "  # Run inference on the batch\n",
    "  output = model(input, attention_mask=masks)[\"logits\"]\n",
    "\n",
    "  # Transfer the output to CPU again and convert to numpy\n",
    "  output = output.cpu().detach().numpy()\n",
    "\n",
    "  # Store the output in a list\n",
    "  outputs.append(output)\n",
    "\n",
    "# Concatenate all the lists within the list into one list\n",
    "outputs = [x for y in outputs for x in y]\n",
    "\n",
    "# Inverse transform the label encoding\n",
    "pred_flat = np.argmax(outputs, axis=1).flatten()\n",
    "output_labels = lable_encoder.inverse_transform(pred_flat)"
   ],
   "metadata": {
    "id": "K-32QySM0Qpu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "submission = pd.DataFrame({\"Id\":np.arange(1, len(output_labels)+1), \"class\":output_labels})\n",
    "# save (submission)\n",
    "submission.to_csv(\"submission23.csv\", index=False)"
   ],
   "metadata": {
    "id": "Az3ccyKv0SYu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "NAo2Q_2F06Je",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "Twitter_Classification.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}